{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c170a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "# from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import pymupdf\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "import sqlite3\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5f3d58",
   "metadata": {},
   "source": [
    "## Filtrando sentenças até a primeira instância"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "anexos = pd.read_excel('docs/jusbrasil/jusbrasil.xlsx', sheet_name=\"Anexos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3cc158ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "anexos_copia = anexos.copy().loc[:,[\"processoID\", \"processoAnexoID\", \"Download copia\", \"Tipo de anexo\", \"Publicado em\"]]\n",
    "sentencas = anexos_copia.loc[anexos[\"Tipo de anexo\"].isin([\"SENTENCA\"]),\n",
    "                                           [\"processoID\", \"processoAnexoID\", \"Download copia\", \"Publicado em\"]]\n",
    "sentencas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d21c08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifica_acesso_negado(processos):\n",
    "    copia_processos = processos.copy()\n",
    "    acessos_negados = []\n",
    "\n",
    "    for i in range(copia_processos.shape[0]):\n",
    "        link = copia_processos.iloc[i, 2]\n",
    "        response = requests.get(link)\n",
    "        response.encoding = 'utf-8'\n",
    "        content_type = response.headers.get('Content-Type', '')\n",
    "        texto = \"\"\n",
    "\n",
    "        if 'html' in content_type:\n",
    "            # print(f\"[HTML] Extraindo de: {link}\")\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            texto = soup.get_text()\n",
    "\n",
    "        match = re.search(r'Acesso negado', texto)\n",
    "        if match:\n",
    "            acessos_negados.append(i)\n",
    "            continue\n",
    "    \n",
    "    return copia_processos.iloc[acessos_negados, :]\n",
    "\n",
    "df_html_only = sentencas[sentencas['Download copia'].str.contains(r'\\.html?$', case=False, na=False)]\n",
    "acessos_negados = identifica_acesso_negado(df_html_only)\n",
    "acessos_negados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropando os acessos negados\n",
    "# sentencas_acessaveis = sentencas[sentencas[\"processoAnexoID\"].isin(acessos_negados[\"processoAnexoID\"]) == False]\n",
    "\n",
    "# Exportando sentenças sem acesso negado para um arquivo Excel\n",
    "# sentencas_acessaveis.to_excel(\"docs/jusbrasil/sentencas_acessaveis.xlsx\", index=False)\n",
    "sentencas_acessaveis = pd.read_excel(\"docs/jusbrasil/sentencas_acessaveis.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ce393d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando quantidade de processos após o drop\n",
    "sentencas_acessaveis[\"processoID\"].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a897b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_processos = sentencas_acessaveis[\"processoID\"].unique()\n",
    "processos_com_mais_de_uma_sentenca = []\n",
    "\n",
    "for id in ids_processos:\n",
    "    linhas_correspondentes = sentencas_acessaveis.loc[sentencas_acessaveis['processoID'] == id]\n",
    "    if linhas_correspondentes.shape[0] > 1:\n",
    "        processos_com_mais_de_uma_sentenca.append(int(id))\n",
    "\n",
    "print(f\"Número de processos com mais de uma sentença: {len(processos_com_mais_de_uma_sentenca)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8dcc087d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencas_final = sentencas_acessaveis.copy()\n",
    "for id in ids_processos:\n",
    "    linhas_correspondentes = sentencas_acessaveis.loc[sentencas_acessaveis['processoID'] == id]\n",
    "    if linhas_correspondentes.shape[0] > 1:\n",
    "        if linhas_correspondentes['Publicado em'].nunique() == 1:\n",
    "            # Data de publicação igual: eliminando por ordem de anexo id\n",
    "            linhas_correspondentes = linhas_correspondentes.sort_values(by=[\"processoAnexoID\"], ascending=True)\n",
    "            manter_id = linhas_correspondentes.iloc[0]['processoAnexoID']\n",
    "        else:\n",
    "            # Ordenando as linhas por data\n",
    "            linhas_correspondentes = linhas_correspondentes.sort_values(by=[\"Publicado em\"], ascending=True)\n",
    "            # Mantendo a primeira linha (mais antiga)\n",
    "            manter_id = linhas_correspondentes.iloc[0]['processoAnexoID']\n",
    "        \n",
    "        # Eliminar todas as outras com mesmo processoID e processoAnexoID diferente do que foi mantido\n",
    "        sentencas_final = sentencas_final[~((sentencas_final[\"processoID\"] == id) & (sentencas_final[\"processoAnexoID\"] != manter_id))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bc1e441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando dataframe após a eliminação\n",
    "ids_processos = sentencas_final[\"processoID\"].unique()\n",
    "processos_com_mais_de_uma_sentenca = []\n",
    "\n",
    "for id in ids_processos:\n",
    "    linhas_correspondentes = sentencas_final.loc[sentencas_final['processoID'] == id]\n",
    "    if linhas_correspondentes.shape[0] > 1:\n",
    "        processos_com_mais_de_uma_sentenca.append(int(id))\n",
    "\n",
    "print(f\"Número de processos com mais de uma sentença: {len(processos_com_mais_de_uma_sentenca)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "abaac9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando se a quantidade de processos após o drop se manteve\n",
    "sentencas_final[\"processoID\"].unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8311aa44",
   "metadata": {},
   "source": [
    "## Extraindo dados das sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc658aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportando df final de sentenças para um arquivo Excel\n",
    "# sentencas_final.to_excel(\"docs/jusbrasil/sentencas_final.xlsx\", index=False)\n",
    "sentencas_final = pd.read_excel(\"docs/jusbrasil/sentencas_final.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e52bf",
   "metadata": {},
   "source": [
    "### Prompt 1 - Filtro: trata-se de um caso de dano ambiental?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44c6232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "respostas = []\n",
    "ids_processos = sentencas_final[\"processoID\"].unique()\n",
    "\n",
    "for id in tqdm(ids_processos):\n",
    "    linha = sentencas_final.loc[sentencas_final['processoID'] == id]\n",
    "    texto = ''\n",
    "    linha = linha.iloc[0] # extrai a linha como Series\n",
    "\n",
    "    processoAnexoID = linha['processoAnexoID']\n",
    "    link = linha['Download copia']\n",
    "    response = requests.get(link)\n",
    "    response.encoding = 'utf-8'\n",
    "    content_type = response.headers.get('Content-Type', '')\n",
    "    \n",
    "    if 'pdf' in content_type:\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
    "            tmp_file.write(response.content)\n",
    "            tmp_path = tmp_file.name\n",
    "\n",
    "        with pymupdf.open(tmp_path) as doc:\n",
    "            for page in doc:\n",
    "                texto += page.get_text()\n",
    "    \n",
    "    elif 'html' in content_type:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        texto += soup.get_text()\n",
    "\n",
    "    resposta_prompt = verifica_dano_ambiental(texto)\n",
    "    resposta_prompt = json.loads(resposta_prompt.text)\n",
    "    resposta_prompt[\"processoAnexoID\"] = processoAnexoID\n",
    "    resposta_prompt[\"link_referencia\"] = link\n",
    "    resposta_prompt[\"processoID\"] = id\n",
    "    respostas.append(resposta_prompt)\n",
    "\n",
    "respostas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc6dfbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando a lista de respostas em um DataFrame\n",
    "respostas_df = pd.DataFrame(respostas)\n",
    "\n",
    "# Reorganizando as colunas\n",
    "respostas_df = respostas_df[[\"processoID\", \"processoAnexoID\", \"isDanoAmbiental\", \"justificativa\", \"link_referencia\"]]\n",
    "\n",
    "respostas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f600e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportando df de classificação de sentenças para um arquivo Excel\n",
    "# respostas_df.to_excel(\"docs/jusbrasil/respostas_classificacao_sentencas.xlsx\", index=False)\n",
    "respostas_df = pd.read_excel(\"docs/jusbrasil/respostas_classificacao_sentencas.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2d0fa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentencas_danos_ambientais = respostas_df.loc[respostas_df[\"isDanoAmbiental\"] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2515c9",
   "metadata": {},
   "source": [
    "### Prompt 2 - Extração dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef446a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuição de processos entre as partes:\n",
      "Parte 1: 19 processos\n",
      "Parte 2: 19 processos\n",
      "Parte 3: 19 processos\n",
      "Parte 4: 22 processos\n",
      "Total de processos: 79\n"
     ]
    }
   ],
   "source": [
    "lista_processos = list(sentencas_danos_ambientais['processoID'].unique())\n",
    "\n",
    "partes = divide_lista_em_partes(lista_processos, 4)\n",
    "print(\"Distribuição de processos entre as partes:\")\n",
    "for i, parte in enumerate(partes):\n",
    "    print(f\"Parte {i + 1}: {len(parte)} processos\")\n",
    "print(\"Total de processos:\", len(lista_processos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "438b4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analisando os processos com dano ambiental\n",
    "respostas_danos_ambientais = []\n",
    "for id in tqdm(lista_processos):\n",
    "    linha = sentencas_danos_ambientais.loc[sentencas_danos_ambientais['processoID'] == id]\n",
    "    texto = ''\n",
    "    linha = linha.iloc[0] # extrai a linha como Series\n",
    "\n",
    "    processoAnexoID = linha['processoAnexoID']\n",
    "    link = linha['link_referencia']\n",
    "    response = requests.get(link)\n",
    "    response.encoding = 'utf-8'\n",
    "    content_type = response.headers.get('Content-Type', '')\n",
    "    \n",
    "    if 'pdf' in content_type:\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
    "            tmp_file.write(response.content)\n",
    "            tmp_path = tmp_file.name\n",
    "\n",
    "        with pymupdf.open(tmp_path) as doc:\n",
    "            for page in doc:\n",
    "                texto += page.get_text()\n",
    "    \n",
    "    elif 'html' in content_type:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        texto += soup.get_text()\n",
    "\n",
    "    resposta_prompt = analisa_sentenca(texto)\n",
    "    resposta_prompt = json.loads(resposta_prompt.text)\n",
    "    resposta_prompt[\"processoAnexoID\"] = processoAnexoID\n",
    "    resposta_prompt[\"link_referencia\"] = link\n",
    "    resposta_prompt[\"processoID\"] = id\n",
    "    respostas_danos_ambientais.append(resposta_prompt)\n",
    "\n",
    "respostas_danos_ambientais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "680eae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformando a lista de respostas de danos ambientais da primeira parte em um DataFrame\n",
    "respostas_danos_ambientais_df_completo = pd.DataFrame(respostas_danos_ambientais)\n",
    "\n",
    "# Reorganizando as colunas\n",
    "respostas_danos_ambientais_df_completo = respostas_danos_ambientais_df_completo[[\"numero_processo\", \"processoID\", \"processoAnexoID\", \"georreferencia\", \"uf\", \"municipio\", \"responsavel\", \"categoria_responsavel\", \"tipo_impacto\", \"descricao_impacto\", \"data_impacto\", \"area_afetada\", \"unidade_area\", \"houve_compensacao\", \"categoria_compensacao\", \"tipo_multa\", \"valor_multa\", \"valor_multa_diaria\", \"link_referencia\"]]\n",
    "\n",
    "respostas_danos_ambientais_df_completo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1496bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# respostas_danos_ambientais_df_completo.to_excel(\"docs/jusbrasil/respostas_danos_ambientais_df_completo.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da3fc20",
   "metadata": {},
   "source": [
    "## Juntando JusBrasil e IOPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3acf7299",
   "metadata": {},
   "outputs": [],
   "source": [
    "respostas_danos_ambientais_df_completo = pd.read_excel(\"docs/jusbrasil/respostas_danos_ambientais_df_completo.xlsx\")\n",
    "df_iopc = pd.read_excel(\"docs/iopc/iopc_tables_final.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c27dd233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coluna de moeda toda preenchida com R$\n",
    "respostas_danos_ambientais_df_completo['moeda'] = 'R$'\n",
    "respostas_danos_ambientais_df_completo['fonte_dados'] = 'JusBrasil'\n",
    "respostas_danos_ambientais_df_completo['pais'] = 'Brasil'\n",
    "# Ordenando as colunas\n",
    "respostas_danos_ambientais_df_completo = respostas_danos_ambientais_df_completo[[\"fonte_dados\", \"numero_processo\", \"processoID\", \"processoAnexoID\", \"georreferencia\", \"pais\", \"uf\", \"municipio\", \"responsavel\", \"categoria_responsavel\", \"tipo_impacto\", \"descricao_impacto\", \"data_impacto\", \"area_afetada\", \"unidade_area\", \"houve_compensacao\", \"categoria_compensacao\", \"tipo_multa\", \"valor_multa\", \"valor_multa_diaria\", \"moeda\", \"link_referencia\"]]\n",
    "respostas_danos_ambientais_df_completo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b923448",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iopc[\"fonte_dados\"] = \"IOPC\"\n",
    "df_iopc.rename(columns={\"Date of Incident\": \"data_impacto\", \n",
    "                        \"Currency\": \"moeda\", \n",
    "                        \"Compensation\": \"valor_multa\", \n",
    "                        \"Estimated quantity of oil spilled (tonnes)\": \"qtde_petroleo_derramada\"}, inplace=True)\n",
    "\n",
    "# Split da coluna Place of Incident em pais e região. Se não encontrar vírgula, coloca NULL em regiao e a informação completa em pais\n",
    "if df_iopc['Place of Incident'].str.contains(',').any():\n",
    "    df_iopc[['regiao', 'pais']] = df_iopc['Place of Incident'].str.split(',', expand=True)\n",
    "else:\n",
    "    df_iopc['regiao'] = None\n",
    "    df_iopc['pais'] = df_iopc['Place of Incident']\n",
    "\n",
    "# Ordenando as colunas\n",
    "df_iopc = df_iopc[[\"fonte_dados\", \"pais\", \"regiao\", \"data_impacto\", \"qtde_petroleo_derramada\", \"valor_multa\", \"moeda\"]]\n",
    "df_iopc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4ad31ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iopc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5aaf26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Juntando os dois DataFrames\n",
    "df_jusbrasil_iopc = pd.concat([respostas_danos_ambientais_df_completo, df_iopc], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75a70e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reordenando as colunas finais\n",
    "df_jusbrasil_iopc = df_jusbrasil_iopc[[\"fonte_dados\", \"numero_processo\", \"processoID\", \"processoAnexoID\", \"georreferencia\", \"pais\", \"uf\", \"municipio\", \"regiao\", \"responsavel\", \"categoria_responsavel\", \"tipo_impacto\", \"descricao_impacto\", \"data_impacto\", \"area_afetada\", \"unidade_area\", \"houve_compensacao\", \"categoria_compensacao\", \"tipo_multa\", \"valor_multa\", \"valor_multa_diaria\", \"moeda\", \"link_referencia\"]]\n",
    "df_jusbrasil_iopc.fillna('NULL', inplace=True)\n",
    "df_jusbrasil_iopc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cb556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jusbrasil_iopc.to_excel(\"docs/df_jusbrasil_iopc.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19460086",
   "metadata": {},
   "source": [
    "## Juntando com o Juscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b2f1b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_juscraper = pd.read_excel(\"docs/juscraper/respostas_danos_ambientais_juscraper.xlsx\")\n",
    "df_juscraper[\"fonte_dados\"] = \"Juscraper\"\n",
    "df_juscraper['moeda'] = 'R$'\n",
    "df_juscraper['pais'] = 'Brasil'\n",
    "df_jusbrasil_iopc_juscraper = pd.concat([df_jusbrasil_iopc, df_juscraper], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab10006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jusbrasil_iopc_juscraper = df_jusbrasil_iopc_juscraper[[\"fonte_dados\", \"numero_processo\", \"processoID\", \"processoAnexoID\", \"georreferencia\", \"pais\", \"uf\", \"municipio\", \"regiao\", \"responsavel\", \"categoria_responsavel\", \"tipo_impacto\", \"descricao_impacto\", \"data_impacto\", \"area_afetada\", \"unidade_area\", \"houve_compensacao\", \"categoria_compensacao\", \"tipo_multa\", \"valor_multa\", \"valor_multa_diaria\", \"moeda\", \"link_referencia\", \"referencia\"]]\n",
    "df_jusbrasil_iopc_juscraper.fillna('NULL', inplace=True)\n",
    "df_jusbrasil_iopc_juscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "683cf8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando se há processos duplicados entre jusbrasil e juscraper\n",
    "df_jusbrasil_juscraper = df_jusbrasil_iopc_juscraper.loc[df_jusbrasil_iopc_juscraper['fonte_dados'] != 'IOPC']\n",
    "\n",
    "for num in df_jusbrasil_juscraper['numero_processo'].unique().tolist():\n",
    "    filtro = df_jusbrasil_juscraper.loc[df_jusbrasil_juscraper['numero_processo'] == num]\n",
    "    if filtro.shape[0] > 1:\n",
    "        display(filtro)\n",
    "        print('*'*100)\n",
    "\n",
    "print(len(df_jusbrasil_juscraper['numero_processo'].tolist()))\n",
    "print(len(df_jusbrasil_juscraper['numero_processo'].unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_jusbrasil_iopc_juscraper.to_excel(\"docs/df_jusbrasil_iopc_juscraper.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e7c3b0",
   "metadata": {},
   "source": [
    "## Extracao Nasser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96eb8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURAÇÕES DO BANCO DE DADOS SQLITE ---\n",
    "DB_FILE_NAME = \"meu_banco_gemini.db\"\n",
    "TABLE_PROCESSOS = \"processos_analisados_gemini\"\n",
    "\n",
    "# Colunas na ordem exata que você especificou para o DataFrame final\n",
    "# Estas são as colunas que você espera que a API Gemini retorne (ou que você preencha)\n",
    "# mais as colunas 'processoID', 'processoAnexoID', 'link_referencia'\n",
    "COLUNAS_DATAFRAME_FINAL = [\n",
    "    \"numero_processo\", \"processoID\", \"processoAnexoID\", \"georreferencia\", \"uf\", \"municipio\",\n",
    "    \"responsavel\", \"categoria_responsavel\", \"tipo_impacto\", \"descricao_impacto\",\n",
    "    \"data_impacto\", \"area_afetada\", \"unidade_area\", \"houve_compensacao\",\n",
    "    \"categoria_compensacao\", \"tipo_multa\", \"valor_multa\", \"valor_multa_diaria\",\n",
    "    \"link_referencia\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15e17972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FUNÇÕES AUXILIARES SQLITE ---\n",
    "\n",
    "def criar_tabela_sqlite():\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(DB_FILE_NAME)\n",
    "        cursor = conn.cursor()\n",
    "        # Criar colunas com tipo TEXT para flexibilidade. processoID será a chave primária.\n",
    "        colunas_sql = \", \".join([f'\"{col}\" TEXT' for col in COLUNAS_DATAFRAME_FINAL if col != \"processoID\"])\n",
    "        # processoID é especial e usado como PRIMARY KEY\n",
    "        cursor.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {TABLE_PROCESSOS} (\n",
    "            \"processoID\" TEXT PRIMARY KEY,\n",
    "            {colunas_sql}\n",
    "        )\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Erro ao criar/conectar tabela SQLite: {e}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "def get_ids_processados_sqlite():\n",
    "    conn = None\n",
    "    ids_processados = set()\n",
    "    # Verificar se o arquivo do banco de dados existe antes de tentar conectar\n",
    "    if not os.path.exists(DB_FILE_NAME):\n",
    "        criar_tabela_sqlite() # Cria a tabela se o DB não existir\n",
    "        return ids_processados\n",
    "\n",
    "    try:\n",
    "        conn = sqlite3.connect(DB_FILE_NAME)\n",
    "        cursor = conn.cursor()\n",
    "        # Verifica se a tabela existe antes de tentar selecionar\n",
    "        cursor.execute(f\"SELECT name FROM sqlite_master WHERE type='table' AND name='{TABLE_PROCESSOS}';\")\n",
    "        if cursor.fetchone() is None:\n",
    "            # Tabela não existe, então crie-a\n",
    "            conn.close() # Fecha a conexão atual para evitar problemas\n",
    "            criar_tabela_sqlite()\n",
    "            return ids_processados # Retorna conjunto vazio pois nada foi processado\n",
    "\n",
    "        cursor.execute(f\"SELECT processoID FROM {TABLE_PROCESSOS}\")\n",
    "        rows = cursor.fetchall()\n",
    "        # Os IDs no seu DataFrame e lista_processos podem ser int ou str.\n",
    "        # Para consistência, vamos tratar como string ao comparar.\n",
    "        ids_processados = {str(row[0]) for row in rows}\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Erro ao ler IDs do SQLite: {e}\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "    return ids_processados\n",
    "\n",
    "def inserir_dados_processo_sqlite(dados_dict):\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(DB_FILE_NAME)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Garantir que todos os campos de COLUNAS_DATAFRAME_FINAL existam no dict, preenchendo com None se faltar\n",
    "        # e convertendo todos os valores para string para inserção segura como TEXT\n",
    "        valores_ordenados = []\n",
    "        for col_nome in COLUNAS_DATAFRAME_FINAL:\n",
    "            valor = dados_dict.get(col_nome)\n",
    "            valores_ordenados.append(str(valor) if valor is not None else None)\n",
    "\n",
    "        cols_string = \", \".join([f'\"{col}\"' for col in COLUNAS_DATAFRAME_FINAL])\n",
    "        placeholders = \", \".join([\"?\"] * len(COLUNAS_DATAFRAME_FINAL))\n",
    "        \n",
    "        # Usar INSERT OR REPLACE para atualizar caso já exista (embora a lógica de pular já deva cobrir)\n",
    "        # ou INSERT OR IGNORE para simplesmente pular se já existir.\n",
    "        # Como estamos checando `ids_processados` antes, um INSERT simples seria suficiente,\n",
    "        # mas OR IGNORE é mais seguro contra race conditions ou lógicas imperfeitas.\n",
    "        cursor.execute(f\"INSERT OR REPLACE INTO {TABLE_PROCESSOS} ({cols_string}) VALUES ({placeholders})\",\n",
    "                       valores_ordenados)\n",
    "        conn.commit()\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Erro ao inserir dados no SQLite para processoID {dados_dict.get('processoID')}: {e}\")\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "def limpar_resposta_json_gemini(texto_json_bruto: str) -> str:\n",
    "    \"\"\"Remove marcadores ```json ... ``` de respostas da API Gemini.\"\"\"\n",
    "    limpo = texto_json_bruto.strip()\n",
    "    if limpo.startswith(\"```json\"):\n",
    "        limpo = limpo[7:]\n",
    "    elif limpo.startswith(\"```\"):\n",
    "        limpo = limpo[3:]\n",
    "    if limpo.endswith(\"```\"):\n",
    "        limpo = limpo[:-3]\n",
    "    return limpo.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bf7bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garante que a tabela exista antes de começar\n",
    "criar_tabela_sqlite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7bde673",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_ja_processados_no_db = get_ids_processados_sqlite()\n",
    "print(f\"Total de IDs na lista original: {len(lista_processos)}\")\n",
    "print(f\"IDs já processados e salvos no banco de dados: {len(ids_ja_processados_no_db)}\")\n",
    "\n",
    "# Filtra a lista_processos para rodar apenas os que ainda não estão no DB\n",
    "# Convertendo os IDs da lista_processos para string para comparação consistente\n",
    "lista_processos_pendentes = [str(pid) for pid in lista_processos if str(pid) not in ids_ja_processados_no_db]\n",
    "\n",
    "print(f\"IDs pendentes para processamento: {len(lista_processos_pendentes)}\")\n",
    "\n",
    "respostas_danos_ambientais = [] # Sua lista original para acumular resultados da rodada atual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5689dfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not lista_processos_pendentes:\n",
    "    print(\"Todos os processos já foram analisados e estão no banco de dados.\")\n",
    "else:\n",
    "    for id_str in tqdm(lista_processos_pendentes, desc=\"Analisando processos pendentes\"):\n",
    "        # O 'id' original pode ser int ou str. Vamos garantir que estamos pegando do DataFrame corretamente.\n",
    "        # Tentamos como string primeiro, depois como int se o DataFrame usar int.\n",
    "        try:\n",
    "            linha = sentencas_danos_ambientais.loc[sentencas_danos_ambientais['processoID'].astype(str) == id_str]\n",
    "        except KeyError: # Se a coluna 'processoID' não for string e falhar a conversão/comparação\n",
    "             # Tenta converter o id_str para o tipo da coluna 'processoID' no DataFrame\n",
    "            id_original_tipo = type(sentencas_danos_ambientais['processoID'].iloc[0])\n",
    "            try:\n",
    "                id_convertido = id_original_tipo(id_str)\n",
    "                linha = sentencas_danos_ambientais.loc[sentencas_danos_ambientais['processoID'] == id_convertido]\n",
    "            except Exception as e_conv:\n",
    "                print(f\"  AVISO: Não foi possível encontrar o processoID '{id_str}' (ou convertido) no DataFrame 'sentencas_danos_ambientais'. Erro de conversão: {e_conv}. Pulando.\")\n",
    "                continue\n",
    "\n",
    "        if linha.empty:\n",
    "            print(f\"  AVISO: ProcessoID '{id_str}' não encontrado em 'sentencas_danos_ambientais'. Pulando.\")\n",
    "            continue\n",
    "\n",
    "        texto_extraido = ''\n",
    "        linha_serie = linha.iloc[0] # extrai a linha como Series\n",
    "\n",
    "        processoAnexoID = linha_serie['processoAnexoID']\n",
    "        link = linha_serie['link_referencia']\n",
    "        \n",
    "        print(f\"\\nProcessando ID: {id_str} | AnexoID: {processoAnexoID} | Link: {link}\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(link, timeout=60) # Timeout maior para downloads\n",
    "            response.raise_for_status() # Levanta exceção para erros HTTP 4xx/5xx\n",
    "            response.encoding = response.apparent_encoding if response.apparent_encoding else 'utf-8' # Melhor detecção de encoding\n",
    "            content_type = response.headers.get('Content-Type', '').lower()\n",
    "            \n",
    "            if 'pdf' in content_type:\n",
    "                # print(\"  Extraindo texto de PDF...\")\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
    "                    tmp_file.write(response.content)\n",
    "                    tmp_path = tmp_file.name\n",
    "                try:\n",
    "                    with pymupdf.open(tmp_path) as doc:\n",
    "                        for page_num, page in enumerate(doc):\n",
    "                            texto_extraido += page.get_text(\"text\") # \"text\" para melhor extração\n",
    "                except Exception as e_pdf:\n",
    "                    print(f\"  ERRO ao processar PDF de {link}: {e_pdf}. Pulando este processo.\")\n",
    "                    if os.path.exists(tmp_path): os.remove(tmp_path)\n",
    "                    continue # Pula para o próximo ID\n",
    "                finally:\n",
    "                    if os.path.exists(tmp_path): os.remove(tmp_path)\n",
    "            \n",
    "            elif 'html' in content_type or 'text/plain' in content_type or not content_type : # Tenta HTML ou texto puro\n",
    "                # print(\"  Extraindo texto de HTML/TEXT...\")\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                # Remove tags de script e style\n",
    "                for script_or_style in soup([\"script\", \"style\"]):\n",
    "                    script_or_style.decompose()\n",
    "                texto_extraido += soup.get_text(separator='\\n', strip=True)\n",
    "            \n",
    "            else: # Se não for PDF nem HTML/TEXT conhecido, tenta ler como texto simples\n",
    "                print(f\"  AVISO: Content-Type '{content_type}' não é PDF nem HTML. Tentando ler como texto direto.\")\n",
    "                texto_extraido += response.text\n",
    "\n",
    "            if not texto_extraido.strip():\n",
    "                print(f\"  AVISO: Nenhum texto foi extraído de {link}. Pulando análise Gemini.\")\n",
    "                continue\n",
    "\n",
    "            # print(f\"  Texto extraído (primeiros 100 chars): {texto_extraido[:100].replace(chr(10), ' ')}...\")\n",
    "            \n",
    "            # Chamada à sua função que interage com a API Gemini\n",
    "            # Esta função deve retornar um objeto com um atributo .text contendo o JSON\n",
    "            resposta_api_obj = analisa_sentenca(texto_extraido) # SUA FUNÇÃO REAL AQUI\n",
    "\n",
    "            # Limpa e parseia o JSON da resposta\n",
    "            json_string_limpo = limpar_resposta_json_gemini(resposta_api_obj.text)\n",
    "            dados_da_api = json.loads(json_string_limpo)\n",
    "            \n",
    "            # Monta o dicionário completo para este processo\n",
    "            # Garante que o 'processoID' seja o mesmo usado para busca (string)\n",
    "            dados_completos_processo = {\"processoID\": str(id_str)} \n",
    "            dados_completos_processo.update(dados_da_api) # Adiciona dados da API\n",
    "            \n",
    "            # Adiciona/sobrescreve as informações que você gerencia manualmente\n",
    "            dados_completos_processo[\"processoAnexoID\"] = processoAnexoID\n",
    "            dados_completos_processo[\"link_referencia\"] = link\n",
    "            \n",
    "            # Adiciona à lista da rodada atual (como no seu código original)\n",
    "            respostas_danos_ambientais.append(dados_completos_processo)\n",
    "            \n",
    "            # Salva no banco de dados SQLite IMEDIATAMENTE\n",
    "            inserir_dados_processo_sqlite(dados_completos_processo)\n",
    "            print(f\"  Processo {id_str} analisado e salvo no banco de dados.\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e_req:\n",
    "            print(f\"  ERRO DE REQUEST ao acessar {link}: {e_req}. Pulando este processo.\")\n",
    "        except json.JSONDecodeError as e_json:\n",
    "            print(f\"  ERRO AO DECODIFICAR JSON da API para o processo {id_str}. Resposta: {getattr(resposta_api_obj, 'text', 'N/A')[:200]}. Erro: {e_json}. Pulando.\")\n",
    "        except AttributeError as e_attr: # Caso analisa_sentenca não retorne .text\n",
    "            print(f\"  ERRO: A função 'analisa_sentenca' não retornou um objeto com atributo '.text' para o processo {id_str}. Erro: {e_attr}. Pulando.\")\n",
    "        except Exception as e_geral:\n",
    "            print(f\"  ERRO INESPERADO ao processar ID {id_str}: {e_geral}. Pulando este processo.\")\n",
    "            # import traceback\n",
    "            # print(traceback.format_exc()) # Para debug mais detalhado se necessário\n",
    "    \n",
    "    print(\"\\nProcessamento dos IDs pendentes concluído.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c0becb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCarregando todos os dados do banco de dados para o DataFrame final...\")\n",
    "conn = None\n",
    "try:\n",
    "    conn = sqlite3.connect(DB_FILE_NAME)\n",
    "    # Lê todas as colunas na ordem definida em COLUNAS_DATAFRAME_FINAL\n",
    "    # Assegura que todas as colunas sejam lidas, mesmo que algumas estejam vazias para alguns registros\n",
    "    col_select_str = \", \".join([f'\"{col}\"' for col in COLUNAS_DATAFRAME_FINAL])\n",
    "    \n",
    "    # Verifica se a tabela existe antes de tentar ler\n",
    "    cursor_check = conn.cursor()\n",
    "    cursor_check.execute(f\"SELECT name FROM sqlite_master WHERE type='table' AND name='{TABLE_PROCESSOS}';\")\n",
    "    if cursor_check.fetchone() is None:\n",
    "        print(f\"A tabela {TABLE_PROCESSOS} não existe no banco de dados. Nenhum dado para carregar.\")\n",
    "        respostas_danos_ambientais_df_completo = pd.DataFrame(columns=COLUNAS_DATAFRAME_FINAL)\n",
    "    else:\n",
    "        respostas_danos_ambientais_df_completo = pd.read_sql_query(f\"SELECT {col_select_str} FROM {TABLE_PROCESSOS}\", conn)\n",
    "except sqlite3.Error as e:\n",
    "    print(f\"Erro ao ler dados do SQLite para o DataFrame: {e}\")\n",
    "    respostas_danos_ambientais_df_completo = pd.DataFrame(columns=COLUNAS_DATAFRAME_FINAL) # Cria DF vazio em caso de erro\n",
    "finally:\n",
    "    if conn:\n",
    "        conn.close()\n",
    "\n",
    "if not respostas_danos_ambientais_df_completo.empty:\n",
    "    # A ordem das colunas já deve estar correta devido ao SELECT explícito,\n",
    "    # mas podemos reconfirmar para garantir, caso a leitura do SQL não preserve 100% a ordem.\n",
    "    respostas_danos_ambientais_df_completo = respostas_danos_ambientais_df_completo[COLUNAS_DATAFRAME_FINAL]\n",
    "    \n",
    "    print(f\"\\nDataFrame final construído a partir do SQLite com {len(respostas_danos_ambientais_df_completo)} registros.\")\n",
    "    print(respostas_danos_ambientais_df_completo.head())\n",
    "\n",
    "    output_excel_file = \"docs/jusbrasil/respostas_danos_ambientais_df_completo.xlsx\"\n",
    "    os.makedirs(\"docs\", exist_ok=True) # Cria o diretório 'docs' se não existir\n",
    "    respostas_danos_ambientais_df_completo.to_excel(output_excel_file, index=False)\n",
    "    print(f\"\\nDataFrame final salvo em: {output_excel_file}\")\n",
    "else:\n",
    "    print(\"\\nNenhum dado foi carregado do banco de dados para gerar o arquivo Excel.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b720bf",
   "metadata": {},
   "source": [
    "# Padronizacao Tipo de Impacto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80390e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jusbrasil_iopc_juscraper = pd.read_excel(\"docs\\df_jusbrasil_iopc_juscraper.xlsx\")\n",
    "lista_impactos_especificos = df_jusbrasil_iopc_juscraper[\"tipo_impacto\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "697fd4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorias_generalizadas = [\n",
    "    'Poluição Hídrica',\n",
    "    'Poluição do Solo',\n",
    "    'Poluição do Ar e Sonora',\n",
    "    'Desmatamento e Danos à Flora',\n",
    "    'Incêndios e Queimadas',\n",
    "    'Danos à Fauna',\n",
    "    'Gestão Inadequada de Resíduos',\n",
    "    'Ocupação e Construção Irregular',\n",
    "    'Erosão, Assoreamento e Impactos Geológicos',\n",
    "    'Extração Ilegal de Recursos Naturais',\n",
    "    'Falhas e Riscos de Infraestrutura',\n",
    "    'Impactos Sociais e à Saúde Pública',\n",
    "    'Danos ao Patrimônio e Bens Públicos',\n",
    "    'Infrações Administrativas e Legais',\n",
    "    'Dano Ambiental Genérico / Outros'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27a0598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
